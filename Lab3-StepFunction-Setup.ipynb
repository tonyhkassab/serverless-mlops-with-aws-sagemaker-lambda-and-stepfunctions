{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Model Retraining & Deployment Using the AWS Step Functions Data Science SDK\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**\n",
    "\n",
    "This notebook describes how to use the AWS Step Functions Data Science SDK to create a machine learning model retraining workflow. The Step Functions SDK is an open source library that allows data scientists to easily create and execute machine learning workflows using AWS Step Functions and Amazon SageMaker. For more information, please see the following resources:\n",
    "* [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "* [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n",
    "* [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io)\n",
    "\n",
    "\n",
    "### Step 0: Get Admin Setup Results\n",
    "Bucket names, codecommit repo, docker image, IAM roles, ...\n",
    "\n",
    "In order to keep things orginized, we will save our `Source Code` (data processing, model training/serving scripts), `datasets`, as well as our trained `model(s) binaries` and their `test-performance metrics` all on S3, **versioned with respect to the date/time of each update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upgrade the stepfunctions library\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from time import gmtime, strftime\n",
    "from stepfunctions.steps.choice_rule import ChoiceRule\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "session = boto3.session.Session()\n",
    "\n",
    "\n",
    "# Set project bucket, IAM Roles and Docker Image for Training\n",
    "with open('admin_setup.txt', 'r') as filehandle:\n",
    "    admin_setup = json.load(filehandle)\n",
    "\n",
    "SOURCE_DATA = admin_setup[\"raw_data_path\"]\n",
    "BUCKET = admin_setup[\"project_bucket\"]\n",
    "REGION = session.region_name\n",
    "\n",
    "TRAINING_IMAGE = admin_setup[\"docker_image\"]\n",
    "WORKFLOW_EXECUTION_ROLE = admin_setup[\"workflow_execution_role\"]\n",
    "\n",
    "REPO = admin_setup[\"repo_name\"]\n",
    "BRANCH = \"master\"\n",
    "\n",
    "WORKFLOW_DATE_TIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "WORKFLOW_NAME = admin_setup[\"repo_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Wrokflow Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_workflow_input_schema = {\n",
    "    #ADMIN\n",
    "    \"REGION\":str,\n",
    "    \"ROLE_ARN\":str,\n",
    "    \"BUCKET\":str,\n",
    "    \"WORKFLOW_NAME\":str,\n",
    "    \"WORKFLOW_DATE_TIME\":str,\n",
    "    \"DATA_SOURCE\":str,\n",
    "    \n",
    "    # CodeCommit\n",
    "    \"REPO\":str,\n",
    "    \"BRANCH\":str,\n",
    "    \"DATA_PROCESSING_DIR\":str,\n",
    "    \"ML_DIR\":str,\n",
    "    \n",
    "    # SM Processing\n",
    "    \"PROCESSING_SCRIPT\":str,\n",
    "    \"PROCESSING_IMAGE\":str,\n",
    "    \"PROCESSING_INSTANCE_TYPE\":str,\n",
    "    \"PROCESSING_INSTANCE_COUNT\":int,\n",
    "    \"PROCESSING_VOLUME_SIZE_GB\":int,\n",
    "    \n",
    "    # SM TRAINING\n",
    "    \"TRAINING_SCRIPT\":str,\n",
    "    \"TRAINING_IMAGE\":str,\n",
    "    \"TRAINING_INSTANCE_TYPE\":str,\n",
    "    \"TRAINING_INSTANCE_COUNT\":int,\n",
    "    \"TRAINING_VOLUME_SIZE_GB\":int,\n",
    "    \n",
    "    # SM SERVING\n",
    "    \"SERVING_SCRIPT\":str,\n",
    "    \"SERVING_IMAGE\":str,\n",
    "    \"SERVING_INSTANCE_TYPE\":str,\n",
    "    \"SERVING_INSTANCE_COUNT\":int,\n",
    "    \"SERVING_VOLUME_SIZE_GB\":int,\n",
    "}\n",
    "my_execution_input = ExecutionInput(schema=my_workflow_input_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StepN: Create Fail State\n",
    "fail_step = steps.states.Fail(\n",
    "    'Workflow Failed',\n",
    "    comment='Either Validation accuracy is lower than threshold or one of processing, training, deployment jobs has faild.'\n",
    ")\n",
    "\n",
    "# Step1: Copy source code from CodeCommit to S3\n",
    "codecommit_to_s3_step = steps.compute.LambdaStep(\n",
    "    state_id = 'Put SourceCode on S3',\n",
    "    parameters={ \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-codecommit-to-s3',\n",
    "        'Payload':{\n",
    "            \"REGION\": my_execution_input[\"REGION\"],\n",
    "            \"BUCKET\": my_execution_input[\"BUCKET\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"],\n",
    "            \"REPO\": my_execution_input[\"REPO\"],\n",
    "            \"BRANCH\": my_execution_input[\"BRANCH\"],\n",
    "            \"ML_DIR\": my_execution_input[\"ML_DIR\"],\n",
    "            \"DATA_PROCESSING_DIR\": my_execution_input[\"DATA_PROCESSING_DIR\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step2: Run SageMaker Data Processing Job\n",
    "data_processing_step = steps.compute.LambdaStep(\n",
    "    state_id = 'Run SageMaker Processing',\n",
    "    parameters={  \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-create-sagemaker-prcoessing-job',\n",
    "        'Payload':{\n",
    "            \"DATA_SOURCE\":SOURCE_DATA,\n",
    "            \"BUCKET\": my_execution_input[\"BUCKET\"],\n",
    "            \"WORKFLOW_NAME\": my_execution_input[\"WORKFLOW_NAME\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"],\n",
    "            \"PROCESSING_INSTANCE_TYPE\": my_execution_input[\"PROCESSING_INSTANCE_TYPE\"],\n",
    "            \"PROCESSING_INSTANCE_COUNT\": my_execution_input[\"PROCESSING_INSTANCE_COUNT\"],\n",
    "            \"PROCESSING_VOLUME_SIZE_GB\": my_execution_input[\"PROCESSING_VOLUME_SIZE_GB\"],\n",
    "            \"PROCESSING_IMAGE\": my_execution_input[\"PROCESSING_IMAGE\"],\n",
    "            \"PROCESSING_SCRIPT\": my_execution_input[\"PROCESSING_SCRIPT\"],\n",
    "            \"ROLE_ARN\": my_execution_input[\"ROLE_ARN\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step3: Wait a little bit\n",
    "wait_for_data_processing = steps.states.Wait(\n",
    "    state_id = \"Wait 30 Seconds\",\n",
    "    seconds = 30\n",
    ")\n",
    "\n",
    "# Step4: Check if processing job has finished\n",
    "get_processing_status = steps.compute.LambdaStep(\n",
    "    state_id = \"Get SageMaker Processing Status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-query-data-processing-status',\n",
    "        'Payload':{\n",
    "            \"WORKFLOW_NAME\": my_execution_input[\"WORKFLOW_NAME\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step5: If processing job is not done, go back to waiting (Step3), if done go to Step6, else go to failure\n",
    "# We will author this step later\n",
    "# ...\n",
    "\n",
    "# Step6: Start SageMaker Training Job\n",
    "model_training_step = steps.compute.LambdaStep(\n",
    "    'Run Model Training Job',\n",
    "    parameters={  \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-create-sagemaker-training-job',\n",
    "        'Payload':{\n",
    "            \"BUCKET\": my_execution_input[\"BUCKET\"],\n",
    "            \"WORKFLOW_NAME\": my_execution_input[\"WORKFLOW_NAME\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"],\n",
    "            \"TRAINING_INSTANCE_TYPE\": my_execution_input[\"TRAINING_INSTANCE_TYPE\"],\n",
    "            \"TRAINING_INSTANCE_COUNT\": my_execution_input[\"TRAINING_INSTANCE_COUNT\"],\n",
    "            \"TRAINING_VOLUME_SIZE_GB\": my_execution_input[\"TRAINING_VOLUME_SIZE_GB\"],\n",
    "            \"TRAINING_IMAGE\": my_execution_input[\"TRAINING_IMAGE\"],\n",
    "            \"TRAINING_SCRIPT\": my_execution_input[\"TRAINING_SCRIPT\"],\n",
    "            \"ROLE_ARN\": my_execution_input[\"ROLE_ARN\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step5: If processing job is not done, go back to waiting (Step3), if done go to Step6, else go to failure\n",
    "check_pocessing_status = steps.states.Choice(\n",
    "    state_id = \"Processing Job Complete?\",\n",
    ")\n",
    "\n",
    "processing_job_output = get_processing_status.output()['Payload']['ProcessingJobStatus']\n",
    "\n",
    "completed_rule = ChoiceRule.StringEquals(variable=processing_job_output, value=\"Completed\")\n",
    "in_progress_rule = ChoiceRule.StringEquals(variable=processing_job_output, value=\"InProgress\")\n",
    "\n",
    "check_pocessing_status.add_choice(rule=completed_rule, next_step=model_training_step)\n",
    "check_pocessing_status.add_choice(rule=in_progress_rule, next_step=wait_for_data_processing)\n",
    "check_pocessing_status.default_choice(fail_step)\n",
    "\n",
    "\n",
    "\n",
    "# Step7: Wait a little bit\n",
    "wait_for_training = steps.states.Wait(\n",
    "    state_id = \"Wait 60 Seconds\",\n",
    "    seconds = 60\n",
    ")\n",
    "\n",
    "# Step8: Check if training job has finished\n",
    "get_training_status = steps.compute.LambdaStep(\n",
    "    state_id = \"Get Training Job Status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-query-training-status',\n",
    "        'Payload':{\n",
    "            \"WORKFLOW_NAME\": my_execution_input[\"WORKFLOW_NAME\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Step9: If training job is not done, go back to waiting (Step7), if done go to Step10, else go to failure\n",
    "# We will author this step later\n",
    "# ...\n",
    "\n",
    "# Step10: Get model accuracy (custom print to logs during training)\n",
    "get_model_accuracy = steps.compute.LambdaStep(\n",
    "    state_id = \"Get Model Median Abs. Err.\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-query-model-accuracy',\n",
    "        'Payload':{\n",
    "            \"WORKFLOW_NAME\": my_execution_input[\"WORKFLOW_NAME\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step9: If training job is not done, go back to waiting (Step7), if done go to Step10, else go to failure\n",
    "check_training_status = steps.states.Choice(\n",
    "    state_id = \"Training Job Complete?\",\n",
    ")\n",
    "\n",
    "training_job_output = get_training_status.output()['Payload']['TrainingJobStatus']\n",
    "\n",
    "completed_rule = ChoiceRule.StringEquals(variable=training_job_output, value=\"Completed\")\n",
    "in_progress_rule = ChoiceRule.StringEquals(variable=training_job_output,value=\"InProgress\")\n",
    "\n",
    "check_training_status.add_choice(rule=completed_rule, next_step=get_model_accuracy)\n",
    "check_training_status.add_choice(rule=in_progress_rule, next_step=wait_for_training)\n",
    "check_training_status.default_choice(fail_step)\n",
    "\n",
    "\n",
    "# Step11: If model's Median Abs. Err. is less than 2, go back to next step (deployment), else go to failure\n",
    "# We will author this step later\n",
    "# ...\n",
    "\n",
    "# Step12: Create Endpoint (or update it if it exists)\n",
    "deploy_model_step = steps.compute.LambdaStep(\n",
    "    'Deploy Model',\n",
    "    parameters={  \n",
    "        \"FunctionName\": WORKFLOW_NAME + '-deploy-sagemaker-model-job',\n",
    "        'Payload':{\n",
    "            \"REGION\": my_execution_input[\"REGION\"],\n",
    "            \"BUCKET\": my_execution_input[\"BUCKET\"],\n",
    "            \"WORKFLOW_NAME\": my_execution_input[\"WORKFLOW_NAME\"],\n",
    "            \"WORKFLOW_DATE_TIME\": my_execution_input[\"WORKFLOW_DATE_TIME\"],\n",
    "            \"SERVING_INSTANCE_TYPE\": my_execution_input[\"SERVING_INSTANCE_TYPE\"],\n",
    "            \"SERVING_INSTANCE_COUNT\": my_execution_input[\"SERVING_INSTANCE_COUNT\"],\n",
    "            \"SERVING_IMAGE\": my_execution_input[\"SERVING_IMAGE\"],\n",
    "            \"SERVING_SCRIPT\": my_execution_input[\"SERVING_SCRIPT\"],\n",
    "            \"ROLE_ARN\": my_execution_input[\"ROLE_ARN\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Step11: If model's Median Abs. Err. is less than 3, go back to next step (deployment), else go to failure\n",
    "check_accuracy_step = steps.states.Choice(\n",
    "    'Median-AE < 3'\n",
    ")\n",
    "mae = get_model_accuracy.output()['Payload']['trainingMetrics'][0]['Value']\n",
    "threshold_rule = ChoiceRule.NumericLessThan(variable=mae, value=3)\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=deploy_model_step)\n",
    "check_accuracy_step.default_choice(next_step=fail_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link all the Steps Together\n",
    "We create a workflow definition by chaining all of the steps together that we've created. See [Chain](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.states.Chain) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Steps 5-16\n",
    "codecommit_to_s3_step.next(data_processing_step)\n",
    "data_processing_step.next(wait_for_data_processing)\n",
    "wait_for_data_processing.next(get_processing_status)\n",
    "get_processing_status.next(check_pocessing_status)\n",
    "model_training_step.next(wait_for_training)\n",
    "wait_for_training.next(get_training_status)\n",
    "get_training_status.next(check_training_status)\n",
    "get_model_accuracy.next(check_accuracy_step)\n",
    "\n",
    "# Chain the whole workflow\n",
    "workflow_definition = steps.Chain([\n",
    "    codecommit_to_s3_step\n",
    "    #wait_for_etl_step,\n",
    "    #get_etl_status,\n",
    "    #check_etl_status\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Workflow(\n",
    "    name=WORKFLOW_NAME,\n",
    "    definition=workflow_definition,\n",
    "    role=WORKFLOW_EXECUTION_ROLE,\n",
    "    execution_input=my_execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your workflow using the workflow definition above, and render the graph with [render_graph](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.render_graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the workflow in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the workflow with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_execution_input_values = {\n",
    "    #ADMIN\n",
    "    \"REGION\":REGION,\n",
    "    \"ROLE_ARN\":WORKFLOW_EXECUTION_ROLE,\n",
    "    \"BUCKET\":BUCKET,\n",
    "    \"WORKFLOW_NAME\": WORKFLOW_NAME,\n",
    "    \"WORKFLOW_DATE_TIME\":WORKFLOW_DATE_TIME,\n",
    "    \"DATA_SOURCE\":SOURCE_DATA,\n",
    "\n",
    "    # CodeCommit\n",
    "    \"REPO\":REPO,\n",
    "    \"BRANCH\":BRANCH,\n",
    "    \"DATA_PROCESSING_DIR\": \"sagemaker-processing-src\",\n",
    "    \"ML_DIR\": \"sagemaker-train-serve-src\",\n",
    "    \n",
    "    # SM Processing\n",
    "    \"PROCESSING_SCRIPT\":\"processing.py\",\n",
    "    \"PROCESSING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"PROCESSING_INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"PROCESSING_INSTANCE_COUNT\":1,\n",
    "    \"PROCESSING_VOLUME_SIZE_GB\":10,\n",
    "    \n",
    "    # SM TRAINING\n",
    "    \"TRAINING_SCRIPT\":\"train.py\",\n",
    "    \"TRAINING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"TRAINING_INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"TRAINING_INSTANCE_COUNT\":1,\n",
    "    \"TRAINING_VOLUME_SIZE_GB\":10,\n",
    "    \n",
    "    # SM SERVING\n",
    "    \"SERVING_SCRIPT\":\"train.py\",\n",
    "    \"SERVING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"SERVING_INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"SERVING_INSTANCE_COUNT\":1,\n",
    "    \"SERVING_VOLUME_SIZE_GB\":10,\n",
    "}\n",
    "\n",
    "execution = workflow.execute(inputs=my_execution_input_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress). This generates a snapshot of the current state of your workflow as it executes. This is a static image therefore you must run the cell again to check progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution.list_events(html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
