{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admin Setup\n",
    "In this notebook we play the role of Data and DevOps engineers who are in charge of maintaining S3 resources (datalake and project buckets) as well as the code repository, Docker image for model training/hosting and IAM credentials respectively.\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**\n",
    "\n",
    "\n",
    "### Step0: Give a title to be used in the name of all project artifacts (code repos, S3 buckets, ML artifacts, Lambda functions, automated pipeline, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_NAME = <Give a name to your project>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create CodeCommit Repo and Push Code to it\n",
    "The cell below creates an AWS CodeCommit repo for this demo. It then adds, commits and pushs our code to this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_session.region_name\n",
    "\n",
    "codecommit_client = boto3.client('codecommit')\n",
    "\n",
    "repo_name = WORKFLOW_NAME\n",
    "repo_desc = \"Automated model (re)training/tuning/hosting via AWS Lambda and Step Functions\"\n",
    "\n",
    "\n",
    "response = codecommit_client.create_repository(\n",
    "    repositoryName=repo_name,\n",
    "    repositoryDescription=repo_desc,\n",
    "    tags={\n",
    "        'author': \"author name\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the above (empty) CodeCommit repo. This repo will be ingested by our pipeline automation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/{repo_name} /home/ec2-user/SageMaker/{repo_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy code to the repo (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ./* /home/ec2-user/SageMaker/{repo_name}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add and Commit code files to local repo, then Push them to master branch of the remot repo on CodeCommit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/{repo_name}/\n",
    "!git add .\n",
    "!git commit -m \"add your comment here...\"\n",
    "\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back to dev folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/serverless-mlops-with-aws-sagemaker-lambda-and-stepfunctions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a toy \"datalake\" S3 and a \"my-project\" bucket\n",
    "The following cell creates two S3 buckets `my-datalake` and `my-project`, feel free to choose whatever name you like. It then:\n",
    "* Uploads our labeled but pre-processed dataset to the `my-datalake` bucket... we will not mess with this bucket, it's job of a data engineer to maintaine it.\n",
    "* We however will work with the `my-project` bucket. We will store everything in it, from our processed datasets to our source code, model binaries and model performance statitics, all of which will be versioned with respect to the date at which the workfolow was launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Create S3 Buckets for this project (pass if they exist)\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "source_bucket = \"{}-datalake-{}\".format(WORKFLOW_NAME, account_id)\n",
    "project_bucket = \"{}-project-{}\".format(WORKFLOW_NAME, account_id)\n",
    "\n",
    "create_bucket(bucket_name=source_bucket, region=None)\n",
    "create_bucket(bucket_name=project_bucket, region=None)\n",
    "print(\"*****************************Storage*******************************\")\n",
    "print(\"Source Bucket: {bucket}\".format(bucket=source_bucket))\n",
    "print(\"Project Bucket: {bucket}\".format(bucket=project_bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload toy data to the source-bucket \"datalake\". Here we use the Boston house prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['PRICE'] = data.target\n",
    "df.to_csv(\"boston.csv\", index=None)\n",
    "data_source = S3Uploader.upload(local_path='boston.csv',\n",
    "                               desired_s3_uri=\"s3://{}/{}\".format(source_bucket, \"data\"),\n",
    "                               #session=session\n",
    "                               )\n",
    "print(\"Source Dataset: {} \\n\".format(data_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Docker image for model training/hosting\n",
    "\n",
    "We have to create a Docker Image for our training container and submit it to Amazon ECR. In this demo, we will use the pre-built [sagemaker-scikit-learn](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-frameworks.html) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Docker Image for Training Job\n",
    "def container_arn(region):\n",
    "    image_registry_map = {\n",
    "        'us-west-1': '746614075791',\n",
    "        'us-west-2': '246618743249',\n",
    "        'us-east-1': '683313688378',\n",
    "        'us-east-2': '257758044811',\n",
    "        'ap-northeast-1': '354813040037',\n",
    "        'ap-northeast-2': '366743142698',\n",
    "        'ap-southeast-1': '121021644041',\n",
    "        'ap-southeast-2': '783357654285',\n",
    "        'ap-south-1': '720646828776',\n",
    "        'eu-west-1': '141502667606',\n",
    "        'eu-west-2': '764974769150',\n",
    "        'eu-central-1': '492215442770',\n",
    "        'ca-central-1': '341280168497',\n",
    "        'us-gov-west-1': '414596584902',\n",
    "        'us-iso-east-1': '833128469047',\n",
    "    }\n",
    "    return (image_registry_map[region] + '.dkr.ecr.' + region \n",
    "            + '.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3')\n",
    "\n",
    "print(\"**************************Training Image***************************\")\n",
    "print(\"Docker Training Image: {} \\n\".format(container_arn(region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create IAM Policies and Roles\n",
    "We'll finally create an IAM role for our workflow. The IAM roles grant the services permissions within your AWS environment. For this demo, we will use the same role that we are using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM Policies and Roles for this project (pass if they exist)\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Set Role/Policy names\n",
    "workflow_role_name = WORKFLOW_NAME+\"-iam-role\"\n",
    "workflow_role_description = \"Role to allow a step function systematic access to invoke Lambda functions, run data processing jobs in Glue, run SageMaker training jobs and update endpoints\"\n",
    "\n",
    "# Get Admin Policy ARN in order to give our workflow role the permission\n",
    "# it needs to run the entire workflow on its own\n",
    "aws_managed_admin_policy_ARN = \"arn:aws:iam::aws:policy/AdministratorAccess\"\n",
    "# Note: This is not a recommended practice in dev/prod evirements. Please work with\n",
    "# your admin on an IAM setup that is addresses your security requirements. \n",
    "\n",
    "\n",
    "# This trust policy allows other services to use this role (lambda/step-functions)\n",
    "trust_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"states.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    },\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\n",
    "            \"Service\": \"sagemaker.amazonaws.com\"\n",
    "        },\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "    },\n",
    "    {\n",
    "      \"Sid\": \"\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"lambda.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Create Step Function Role (pass if it already exists)\n",
    "try:\n",
    "    step_function_role = iam.create_role(\n",
    "        RoleName=workflow_role_name,\n",
    "        Description=workflow_role_description,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Unable to create {}\".format(workflow_role_name))\n",
    "\n",
    "    \n",
    "# Attach the Admin Full Access Policy\n",
    "try:\n",
    "    iam.attach_role_policy(\n",
    "        PolicyArn=aws_managed_admin_policy_ARN,\n",
    "        RoleName=workflow_role_name\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Unable to attach {} to {}\".format(aws_managed_admin_policy_ARN, workflow_role_name))\n",
    "\n",
    "\n",
    "# Get Role ARN in order to print\n",
    "workflow_role_ARN = \"arn:aws:iam::{account}:role/{name}\".format(account=account_id, name=workflow_role_name)\n",
    "print(\"Step Function Role ARN: {arn}\".format(arn = workflow_role_ARN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the setup ARNs to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_setup = {\n",
    "    \"workflow_name\": WORKFLOW_NAME,\n",
    "    \"source_bucket\":source_bucket,\n",
    "    \"raw_data_path\":data_source,\n",
    "    \"project_bucket\":project_bucket,\n",
    "    \"repo_name\":repo_name,\n",
    "    \"docker_image\": container_arn(region),\n",
    "    \"workflow_execution_role\": workflow_role_ARN\n",
    "}\n",
    "\n",
    "with open('admin_setup.txt', 'w') as filehandle:\n",
    "    filehandle.write(json.dumps(admin_setup))\n",
    "    \n",
    "admin_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
