{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker SDK Demo\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**\n",
    "\n",
    "### Step 0: Get Admin Setup Results\n",
    "Bucket names, codecommit repo, docker image, IAM roles, ...\n",
    "\n",
    "In order to keep things orginized, we will save our `Source Code` (data processing, model training/serving scripts), `datasets`, as well as our trained `model(s) binaries` and their `test-performance metrics` all on S3, **versioned with respect to the date/time of each update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "# Grab admin resources (S3 Bucket name, IAM Roles and Docker Image for Training)\n",
    "with open('admin_setup.txt', 'r') as filehandle:\n",
    "    admin_setup = json.load(filehandle)\n",
    "\n",
    "SOURCE_DATA = admin_setup[\"raw_data_path\"]\n",
    "BUCKET = admin_setup[\"project_bucket\"]\n",
    "REPO_NAME = admin_setup[\"repo_name\"]\n",
    "WORKFLOW_NAME = admin_setup[\"workflow_name\"]\n",
    "WORKFLOW_DATE_TIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our data processing script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./sagemaker-processing-src/processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir data/input\n",
    "!mkdir data/train\n",
    "!mkdir data/validation\n",
    "!mkdir data/test\n",
    "!cp boston.csv data/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./sagemaker-processing-src/processing.py --local_path ./data/\n",
    "\n",
    "\n",
    "!ls data/train/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "INPUT_DESTINATION = SOURCE_DATA\n",
    "OUTPUT_DESTINATION = 's3://{}/{}/data'.format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "PROCESSING_JOB_NAME = \"dev-project-data-processing-{}\".format(WORKFLOW_DATE_TIME)\n",
    "\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1\n",
    "                                    )\n",
    "\n",
    "inputs = [ProcessingInput(source=INPUT_DESTINATION,\n",
    "                          destination='/opt/ml/processing/input',\n",
    "                          s3_data_distribution_type='ShardedByS3Key'\n",
    "                         )\n",
    "         ]\n",
    "\n",
    "outputs = [ProcessingOutput(output_name='train',\n",
    "                            destination='{}/train'.format(OUTPUT_DESTINATION),\n",
    "                            source='/opt/ml/processing/train'\n",
    "                           ),\n",
    "           ProcessingOutput(output_name='validation',\n",
    "                            destination='{}/validation'.format(OUTPUT_DESTINATION),\n",
    "                            source='/opt/ml/processing/validation'\n",
    "                           ),\n",
    "           ProcessingOutput(output_name='test',\n",
    "                            destination='{}/test'.format(OUTPUT_DESTINATION),\n",
    "                            source='/opt/ml/processing/test'\n",
    "                           )\n",
    "          ]\n",
    "\n",
    "sklearn_processor.run(code='./sagemaker-processing-src/processing.py',\n",
    "                      job_name = PROCESSING_JOB_NAME,\n",
    "                      inputs = inputs,\n",
    "                      outputs = outputs\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {\"s3://{}/{}/data/\".format(BUCKET, WORKFLOW_DATE_TIME)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./sagemaker-train-serve-src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./models\n",
    "\n",
    "%run -i sagemaker-train-serve-src/train.py \\\n",
    "    --model-dir ./models \\\n",
    "    --train ./data/train/ \\\n",
    "    --validation ./data/validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the SageMaker SKLearn Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import sagemaker\n",
    "\n",
    "# regex to extract our objective metric from training job logs\n",
    "validation_metric_defs = [{'Name': 'median_ae',\n",
    "                           'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"\n",
    "                          }]\n",
    "\n",
    "# Instantiate estimator\n",
    "train_estimator = SKLearn(sagemaker_session = sagemaker.Session(),\n",
    "                          role = role,\n",
    "                          source_dir = './sagemaker-train-serve-src/',\n",
    "                          entry_point = 'train.py',\n",
    "                          instance_type = 'ml.m5.xlarge',\n",
    "                          instance_count = 1,\n",
    "                          framework_version = '0.23-1',\n",
    "                          metric_definitions = validation_metric_defs,\n",
    "                          output_path = 's3://{}/{}'.format(BUCKET, WORKFLOW_DATE_TIME + '/model-artifacts'),\n",
    "                          code_location = 's3://{}/{}'.format(BUCKET, WORKFLOW_DATE_TIME + '/source-code')\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 data paths\n",
    "TRAINING_DATA_PATH = \"s3://{}/{}/data/train/train.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "VALIDATION_DATA_PATH = \"s3://{}/{}/data/validation/validation.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "TESTING_DATA_PATH = \"s3://{}/{}/data/test/test.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "\n",
    "\n",
    "train_estimator.fit(job_name = \"{}-{}-sdk\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME),\n",
    "                    inputs = {\"train\" : TRAINING_DATA_PATH,\n",
    "                              \"validation\" : VALIDATION_DATA_PATH\n",
    "                             },\n",
    "                    wait = True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Built-in](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) XGBoost Algorithms Example: no training code required\n",
    "\n",
    "#### Note: For CSV training datasets, the algorithm assumes that the target variable is in the first column and that the CSV does not have a header record. \n",
    "\n",
    "``` python\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri \n",
    "from sagemaker.session import s3_input, Session\n",
    "region = boto3.Session().region_name\n",
    "xgboost_container = get_image_uri(region, 'xgboost', repo_version='latest')\n",
    "\n",
    "# Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":\"50\"\n",
    "}\n",
    "\n",
    "# Construct a SageMaker estimator that calls the xgboost-container\n",
    "builtin_xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    image_name=xgboost_container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.m5.2xlarge', \n",
    "    train_volume_size=5, \n",
    "    output_path='s3://{}/{}'.format(BUCKET, WORKFLOW_DATE_TIME + '/builtin-xgb-model-artifacts')\n",
    ")\n",
    "\n",
    "\n",
    "# Execute the XGBoost training job\n",
    "train_input = s3_input(\"s3://...\", content_type=\"csv\")\n",
    "validation_input = s3_input(\"s3://...\", content_type=\"csv\")\n",
    "builtin_xgb_estimator.fit({'train': train_input, 'validation': validation_input}, wait=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter\n",
    "\n",
    "# Define exploration boundaries\n",
    "hyperparameter_ranges = {\n",
    "    'n-estimators': IntegerParameter(20, 100),\n",
    "    'min-samples-leaf': IntegerParameter(2, 6)\n",
    "}\n",
    "\n",
    "# Create Optimizer\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    base_tuning_job_name=\"{}-tuner-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME),\n",
    "    estimator=train_estimator,\n",
    "    max_jobs=12,\n",
    "    max_parallel_jobs=4,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    objective_metric_name='median-AE',\n",
    "    objective_type='Minimize',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'median-AE',\n",
    "         'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"\n",
    "        }] # extract tracked metric from logs with regexp   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Optimizer\n",
    "Optimizer.fit({'train': TRAINING_DATA_PATH, 'validation': VALIDATION_DATA_PATH})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Optimizer results in a df\n",
    "results = Optimizer.analytics().dataframe()\n",
    "results.sort_values(\"FinalObjectiveValue\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker hosted endpoint\n",
    "\n",
    "We can easily deploy a SageMaker model to production. A convenient option is to use a SageMaker hosted endpoint, which serves real time predictions from the trained model (Batch Transform jobs also are available for asynchronous, offline predictions on large datasets). The endpoint will retrieve the SavedModel created during training and deploy it within a SageMaker SKLearn container. This all can be accomplished with one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator.deploy(initial_instance_count=1,\n",
    "                       instance_type=\"ml.m5.xlarge\",\n",
    "                       endpoint_name=WORKFLOW_NAME,\n",
    "                       wait=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If hyperparameter optimization was used, we can deploy the best model from the HyperparameterTuner directly\n",
    "By calling the deploy method of the HyperparameterTuner object we instantiated above, we can directly deploy the best model from the tuning job to a SageMaker hosted endpoint.\n",
    "\n",
    "```python\n",
    "Optimizer.deploy(initial_instance_count=1,\n",
    "                 instance_type=\"ml.m5.xlarge\",\n",
    "                 endpoint_name = \"dev-project-tuned-model\"\n",
    "                )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on cost:\n",
    "The above training job took 100 seconds on a `ml.m5.2xlarge` which [costs](https://aws.amazon.com/sagemaker/pricing/) `$0.538` per hour. But let's assume that training the model took 30 minutes instead, and that the data processing job took 60 minutes. \n",
    "\n",
    "Furthermore, if we assume that we will host our model on an `ml.c5.large` instance (2 CPUs /4 GiB Memory), the cost per hour for this instance is `$0.119`. With that, our total cost will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_notebook_cost = 173.33*0.0582\n",
    "print(\"monthly_notebook_cost = $\" + str(monthly_notebook_cost))\n",
    "\n",
    "processing_cost_per_sec = 0.538/3600\n",
    "training_cost_per_sec = 0.538/3600\n",
    "monthly_cycles = 4\n",
    "\n",
    "processing_secs = 60*60\n",
    "processing_job_cost = processing_cost_per_sec*processing_secs\n",
    "processing_jobs_cost = monthly_cycles*processing_job_cost\n",
    "print(\"processing_jobs_cost = $\" + str(processing_jobs_cost))\n",
    "\n",
    "training_secs = 60*30\n",
    "training_job_cost = training_cost_per_sec*training_secs\n",
    "training_jobs_cost = monthly_cycles*training_job_cost\n",
    "print(\"training_jobs_cost = $\" + str(training_jobs_cost))\n",
    "\n",
    "hosting_cost = 0.119*24*30\n",
    "print(\"hosting_cost = $\" + str(hosting_cost))\n",
    "\n",
    "total_mothly_cost = monthly_notebook_cost + processing_jobs_cost + training_jobs_cost + hosting_cost\n",
    "print(\"total_mothly_cost = $\" + str(round(total_mothly_cost,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test our endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "endpoint_status = \"Creating\"\n",
    "while endpoint_status != \"InService\":\n",
    "    endpoint_status = sagemaker_client.describe_endpoint(**{'EndpointName': WORKFLOW_NAME})[\"EndpointStatus\"]\n",
    "    print(endpoint_status)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['PRICE'] = data.target\n",
    "print(df.shape)\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=WORKFLOW_NAME,\n",
    "    Body=df[data.feature_names].to_csv(header=False, index=False).encode('utf-8'),\n",
    "    ContentType='text/csv')\n",
    "\n",
    "decoded_response = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "print(decoded_response[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native support for data-capture\n",
    "```python\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture = True,\n",
    "                        sampling_percentage=50,\n",
    "                        destination_s3_uri='s3://.../',\n",
    "                        kms_key_id=None,\n",
    "                        capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "                        csv_content_types=[\"text/csv\"],\n",
    "                        json_content_types=[\"application/json\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Attach the new configuration to your endpoint...\n",
    "\n",
    "```python\n",
    "from sagemaker import RealTimePredictor\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=\"dev-project\")\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform job (without a real-time endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "transformer = train_estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')\n",
    "transformer.transform(\"<path to input data>\", content_type='text/csv')\n",
    "\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "batch_output = transformer.output_path\n",
    "print(batch_output)\n",
    "```\n",
    "\n",
    "#### Or launch transform jobs from anywhere using boto3\n",
    "```python\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "response = sm_client.create_transform_job(\n",
    "    TransformJobName = \"{}-batch-transform-{}\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME),\n",
    "    ModelName = \"{}-{}-sdk\".format(WORKFLOW_NAME, WORKFLOW_DATE_TIME)\n",
    "    TransformInput = {\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': TESTING_DATA_PATH\n",
    "            }\n",
    "        },\n",
    "        'ContentType': 'text/csv',\n",
    "        'CompressionType': 'None',\n",
    "        'SplitType': 'Line'\n",
    "    },\n",
    "    TransformOutput = {\n",
    "        'S3OutputPath': \"s3://{}/{}/data/test/test_pred.csv\".format(BUCKET, WORKFLOW_DATE_TIME)\n",
    "    },\n",
    "    TransformResources = {\n",
    "        'InstanceType': 'ml.m5.large',\n",
    "        'InstanceCount': 1\n",
    "    }\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of deploying models trained outside SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "model_artifact_on_s3 = sm_client.describe_training_job(\n",
    "    TrainingJobName=train_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Model artifact persisted at ' + model_artifact_on_s3)\n",
    "\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "model = SKLearnModel(model_data=model_artifact_on_s3,\n",
    "                     role=role,\n",
    "                     source_dir = './sagemaker-train-serve-src/',\n",
    "                     entry_point='train.py'\n",
    "                    )\n",
    "\n",
    "model.deploy(instance_type='ml.c5.large',\n",
    "             initial_instance_count=1,\n",
    "             endpoint_name='dev-project',\n",
    "             wait=False\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Custom Inference Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from six import BytesIO\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "MODEL_NAME = 'octank_model.joblib'\n",
    "TARGET_NORMALIZER_NAME = 'octank_target_normalizer.joblib'\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Part of the SageMaker sklearn docker image, this function loads the serialized model.\n",
    "    More information: https://sagemaker.readthedocs.io/en/stable/using_sklearn.html\n",
    "    \n",
    "    Args:\n",
    "        model_dir: a directory where model is saved.\n",
    "    Returns: \n",
    "        a tuple of two Scikit-learn models.\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, MODEL_NAME))\n",
    "    target_normalizer = joblib.load(os.path.join(model_dir, TARGET_NORMALIZER_NAME))\n",
    "    return (model, target_normalizer)\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\" This function is called on the byte stream sent by the client, and is used to deserialize the\n",
    "    bytes into a Python object suitable for inference by predict_fn.  -- in this case, a NumPy array.\n",
    "    More information: https://sagemaker.readthedocs.io/en/stable/using_sklearn.html\n",
    "\n",
    "    This implementation assumes users will want to run predict on a pandas dataframe. The will first \n",
    "    serialize the datafrme using _npy_dumps() and then request a prediction. _npy_load() is in charge\n",
    "    of deserializing the data. \n",
    "\n",
    "    Args:\n",
    "        input_bytes (numpy array): a numpy array containing the data serialized by the Chainer predictor\n",
    "        content_type: the MIME type of the data in input_bytes\n",
    "    Returns:\n",
    "        a NumPy array represented by input_bytes.\n",
    "    \"\"\"    \n",
    "    if content_type == 'application/json':\n",
    "        # Read the raw input data as json.\n",
    "        input_dict = json.loads(input_data)\n",
    "        \n",
    "        # Convert to Pandas DF\n",
    "        input_df = pd.DataFrame(input_dict, index=[0])\n",
    "\n",
    "        return input_df\n",
    "    \n",
    "    elif content_type == 'text/csv':\n",
    "        data = csv.reader(input_data.splitlines()[1:])\n",
    "        data = pd.DataFrame(data)\n",
    "        data.columns = input_data.splitlines()[0].split(',')\n",
    "        return data.replace(\"\",0)\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "        \n",
    "def predict_fn(input_data, models):\n",
    "    \"\"\"Part of the sklearn docker image, this function takes the deserialized request object\n",
    "    and performs inference against the loaded model(s).\n",
    "    More information: https://sagemaker.readthedocs.io/en/stable/using_sklearn.html.\n",
    "\n",
    "    Args:\n",
    "        input_data: the return value from input_fn()\n",
    "        model: tuple with loaded models from model_fn(). First element\n",
    "               is the pipeline_model and the second is the target\n",
    "               transformer    \n",
    "    Returns:\n",
    "        Predictions in numpy array.\n",
    "    \"\"\"\n",
    "    # Parse models\n",
    "    model, target_normalizer = models\n",
    "    # Grab model feature list\n",
    "    model_features = model.named_steps['algo'].get_booster().feature_names\n",
    "    \n",
    "    # Lower case feature names\n",
    "    input_data.columns = [x.lower() for x in input_data.columns.values.tolist()]\n",
    "    cols = input_data.columns.values.tolist()\n",
    "    \n",
    "    for f in model_features:\n",
    "        if f not in cols:\n",
    "            input_data[f] = 0\n",
    "        else:\n",
    "            input_data[f] = input_data[f].astype('float')\n",
    "        \n",
    "    # Predict\n",
    "    input_data = input_data[model_features]\n",
    "    \n",
    "    y_pred_link = model.predict(input_data).reshape(-1,1)\n",
    "    y_pred = target_normalizer.inverse_transform(y_pred_link).ravel()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def _npy_dumps(data):\n",
    "    \"\"\"Serializes a numpy array into a stream of npy-formatted bytes.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    np.save(buffer, data)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def output_fn(prediction_output, accept):\n",
    "    if accept == 'application/x-npy' or accept == 'application/json':\n",
    "        print('output_fn input is', prediction_output, 'in format', accept)\n",
    "        return _npy_dumps(prediction_output), 'application/x-npy'\n",
    "    else:\n",
    "        raise ValueError('Accept header must be application/x-npy or application/json, but it is {}'.format(accept))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
