{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run and end-to-end ML pipeline using AWS SageMaker and Lambda\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**\n",
    "\n",
    "### Step 0: Get Admin Setup Results\n",
    "Bucket names, codecommit repo, docker image, IAM roles, ...\n",
    "\n",
    "In order to keep things orginized, we will save our `Source Code` (data processing, model training/serving scripts), `datasets`, as well as our trained `model(s) binaries` and their `test-performance metrics` all on S3, **versioned with respect to the date/time of each update.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import zipfile\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.s3 import S3Uploader\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Grab admin resources (S3 Bucket name, IAM Roles and Docker Image for Training)\n",
    "with open('admin_setup.txt', 'r') as filehandle:\n",
    "    admin_setup = json.load(filehandle)\n",
    "\n",
    "# MLOps Hygiene\n",
    "BUCKET = admin_setup[\"project_bucket\"]\n",
    "SOURCE_DATA = admin_setup[\"raw_data_path\"]\n",
    "BRANCH = \"master\"\n",
    "REPO = admin_setup[\"repo_name\"]\n",
    "WORKFLOW_NAME = admin_setup[\"workflow_name\"]\n",
    "\n",
    "REGION = session.region_name\n",
    "TRAINING_IMAGE = admin_setup[\"docker_image\"]\n",
    "WORKFLOW_EXECUTION_ROLE = admin_setup[\"workflow_execution_role\"]\n",
    "WORKFLOW_DATE_TIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "SOURCE_CODE_PREFIX = \"{}/{}\".format(WORKFLOW_DATE_TIME, \"source-code\")\n",
    "\n",
    "my_workflow_input = {\n",
    "    #ADMIN\n",
    "    \"REGION\":REGION,\n",
    "    \"ROLE_ARN\":WORKFLOW_EXECUTION_ROLE,\n",
    "    \"BUCKET\":BUCKET,\n",
    "    \"WORKFLOW_NAME\":WORKFLOW_NAME,\n",
    "    \"WORKFLOW_DATE_TIME\":WORKFLOW_DATE_TIME,\n",
    "    \"DATA_SOURCE\":SOURCE_DATA,\n",
    "\n",
    "    # CodeCommit\n",
    "    \"REPO\":REPO,\n",
    "    \"BRANCH\":BRANCH,\n",
    "    \"DATA_PROCESSING_DIR\": \"sagemaker-processing-src\",\n",
    "    \"ML_DIR\": \"sagemaker-train-serve-src\",\n",
    "    \n",
    "    # SM Processing\n",
    "    \"PROCESSING_SCRIPT\":\"processing.py\",\n",
    "    \"PROCESSING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"PROCESSING_INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"PROCESSING_INSTANCE_COUNT\":1,\n",
    "    \"PROCESSING_VOLUME_SIZE_GB\":10,\n",
    "    \n",
    "    # SM TRAINING\n",
    "    \"TRAINING_SCRIPT\":\"train.py\",\n",
    "    \"TRAINING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"TRAINING_INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"TRAINING_INSTANCE_COUNT\":1,\n",
    "    \"TRAINING_VOLUME_SIZE_GB\":10,\n",
    "    \n",
    "    # SM SERVING\n",
    "    \"SERVING_SCRIPT\":\"train.py\",\n",
    "    \"SERVING_IMAGE\":TRAINING_IMAGE,\n",
    "    \"SERVING_INSTANCE_TYPE\":\"ml.c5.xlarge\",\n",
    "    \"SERVING_INSTANCE_COUNT\":1,\n",
    "    \"SERVING_VOLUME_SIZE_GB\":10,\n",
    "}\n",
    "\n",
    "# The following method will be used throughout this notebook to create Lambda functions without going to the console\n",
    "session = sagemaker.Session()\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "def create_lambda_function(zip_name, lambda_source_code, function_name, description):\n",
    "    zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "    zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "    zf.close()\n",
    "\n",
    "    S3Uploader.upload(local_path=zip_name, \n",
    "                      desired_s3_uri=\"s3://{}/{}\".format(BUCKET, SOURCE_CODE_PREFIX),\n",
    "                      #session=session\n",
    "                     )\n",
    "\n",
    "    response = lambda_client.create_function(\n",
    "        FunctionName=function_name,\n",
    "        Runtime='python3.6',\n",
    "        Role=WORKFLOW_EXECUTION_ROLE,\n",
    "        Handler=zip_name.split('.')[0]+'.lambda_handler',\n",
    "        Code={\n",
    "            'S3Bucket': BUCKET,\n",
    "            'S3Key': '{}/{}'.format(SOURCE_CODE_PREFIX, zip_name)\n",
    "        },\n",
    "        Description=description,\n",
    "        Timeout=180,\n",
    "        MemorySize=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Move Code from CodeCommit to S3\n",
    "The first step in training a model on sagemaker is to copy our source code to S3. This step is automatically done for you when you use the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./workflow-orchestration-src/codecommit_to_s3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the above script from a Lambda function, this will help us automoate this task later.\n",
    "\n",
    "First create the Lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name=\"codecommit_to_s3.zip\",\n",
    "                       lambda_source_code=\"./workflow-orchestration-src/codecommit_to_s3.py\",\n",
    "                       function_name=WORKFLOW_NAME + '-codecommit-to-s3',\n",
    "                       description=\"Copy code files from CodeCommit to a tarball on S3\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.invoke(\n",
    "    FunctionName=WORKFLOW_NAME + '-codecommit-to-s3',\n",
    "    Payload=json.dumps(my_workflow_input).encode()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"Payload\"].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {\"s3://{}/{}/source-code/\".format(BUCKET, WORKFLOW_DATE_TIME)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run SageMaker Processing Job with `boto3`\n",
    "\n",
    "The `boto3` client for SageMaker is more verbose than the SageMaker SDK yet gives more visibility in the low-level details of Amazon SageMaker.\n",
    "\n",
    "To run a custom data processing script on SageMaker, we will use [boto3.client('sagemaker')\n",
    ".create_processing_job()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_processing_job) inside a lambda function.\n",
    "\n",
    "Here is the code for the lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./workflow-orchestration-src/create_sagemaker_prcoessing_job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name=\"create_sagemaker_prcoessing_job.zip\",\n",
    "                       lambda_source_code=\"./workflow-orchestration-src/create_sagemaker_prcoessing_job.py\",\n",
    "                       function_name=WORKFLOW_NAME + '-create-sagemaker-prcoessing-job',\n",
    "                       description=\"Creates Sagemaker Processing Job\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.invoke(\n",
    "    FunctionName=WORKFLOW_NAME + '-create-sagemaker-prcoessing-job',\n",
    "    Payload=json.dumps(my_workflow_input).encode()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"Payload\"].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a mechanism to check on the processing job status... again using a Lambda!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name='query_data_processing_status.zip',\n",
    "                       lambda_source_code='./workflow-orchestration-src/query_data_processing_status.py',\n",
    "                       function_name=WORKFLOW_NAME + '-query-data-processing-status',\n",
    "                       description='Get Status of SageMaker Processing Job'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure the SageMaker processing job is done (status = Completed) before luanching the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%cd ./workflow-orchestration-src\n",
    "import query_data_processing_status as qs\n",
    "\n",
    "while qs.lambda_handler(my_workflow_input, \"\")[\"ProcessingJobStatus\"] == \"InProgress\":\n",
    "    print(qs.lambda_handler(my_workflow_input, \"\")[\"ProcessingJobStatus\"])\n",
    "    time.sleep(5)\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {\"s3://{}/{}/\".format(BUCKET, WORKFLOW_DATE_TIME)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create SageMaker Training Job Using `boto3`\n",
    "\n",
    "When using `boto3` to launch a training job, we must explicitly point it to our source code on S3 and docker image in addition to what SageMaker estimators expect.\n",
    "\n",
    "Let's look at the code for the `create_sagemaker_training_job` lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./workflow-orchestration-src/create_sagemaker_training_job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name='create_sagemaker_training_job.zip',\n",
    "                       lambda_source_code='./workflow-orchestration-src/create_sagemaker_training_job.py',\n",
    "                       function_name=WORKFLOW_NAME + '-create-sagemaker-training-job',\n",
    "                       description='Creates SageMaker Training Job'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training job once processing job is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.invoke(\n",
    "    FunctionName=WORKFLOW_NAME + '-create-sagemaker-training-job',\n",
    "    Payload=json.dumps(my_workflow_input).encode()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"Payload\"].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check on its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name='query_training_status.zip',\n",
    "                       lambda_source_code='./workflow-orchestration-src/query_training_status.py',\n",
    "                       function_name=WORKFLOW_NAME + '-query-training-status',\n",
    "                       description='Get Status of SageMaker Training Job'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure the SageMaker training job is done (status = Completed) before deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./workflow-orchestration-src\n",
    "\n",
    "import query_training_status as qs\n",
    "\n",
    "while qs.lambda_handler(my_workflow_input, \"\")[\"TrainingJobStatus\"] == \"InProgress\":\n",
    "    print(qs.lambda_handler(my_workflow_input, \"\")[\"TrainingJobStatus\"])\n",
    "    time.sleep(5)\n",
    "    \n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {\"s3://{}/{}/\".format(BUCKET, WORKFLOW_DATE_TIME)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy model on SageMaker using model artifacts on S3 using `boto3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If training is done, then check model accuracy before deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name='query_model_accuracy.zip',\n",
    "                       lambda_source_code='./workflow-orchestration-src/query_model_accuracy.py',\n",
    "                       function_name=WORKFLOW_NAME + '-query-model-accuracy',\n",
    "                       description='Get Model Accuracy from SageMaker Training Job'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./workflow-orchestration-src\n",
    "\n",
    "import query_model_accuracy as qs\n",
    "print(qs.lambda_handler(my_workflow_input, \"\"))\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the code for the `deploy_sagemaker_model` lambda function. This function will be incharge of creating a SageMaker endpoint for our trained model. If endpoint exists, then it will update the endpoint with the new retrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./workflow-orchestration-src/deploy_sagemaker_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's put this function in a Lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lambda_function(zip_name='deploy_sagemaker_model.zip',\n",
    "                       lambda_source_code='./workflow-orchestration-src/deploy_sagemaker_model.py',\n",
    "                       function_name=WORKFLOW_NAME + '-deploy-sagemaker-model-job',\n",
    "                       description='Creates and Deploys SageMaker Model From Training Artifacts'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.invoke(\n",
    "    FunctionName=WORKFLOW_NAME + '-deploy-sagemaker-model-job',\n",
    "    Payload=json.dumps(my_workflow_input).encode()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"Payload\"].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "endpoint_status = \"Creating\"\n",
    "while endpoint_status != \"InService\":\n",
    "    endpoint_status = sagemaker_client.describe_endpoint(**{'EndpointName': WORKFLOW_NAME})[\"EndpointStatus\"]\n",
    "    print(endpoint_status)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import json\n",
    "#session = sagemaker.Session()\n",
    "data = load_boston()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['PRICE'] = data.target\n",
    "print(df.shape)\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=WORKFLOW_NAME,\n",
    "    Body=df[data.feature_names].to_csv(header=False, index=False).encode('utf-8'),\n",
    "    ContentType='text/csv')\n",
    "\n",
    "decoded_response = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "print(decoded_response[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
